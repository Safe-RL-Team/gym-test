{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import time\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "env_name = \"LunarLander-v2\"  # name of the environment\n",
    "n = (4,8)  # weight matrix size\n",
    "runs = 15  # runs per weights to get a good average\n",
    "simulation_len = 300  # number of steps in one simulation run\n",
    "search_len = 100  # number of weights to try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seach best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup environment\n",
    "env = gym.make(env_name)\n",
    "observation = env.reset()\n",
    "\n",
    "# setup best weights\n",
    "w_best = np.zeros(n)\n",
    "best_score = -np.inf\n",
    "\n",
    "# try many random weights\n",
    "for i in range(search_len):\n",
    "\n",
    "    # log\n",
    "    print(f'\\rtry {i+1}/{search_len}', end='')\n",
    "\n",
    "    # generate random neuron weights\n",
    "    w = np.random.uniform(-1, 1, n)\n",
    "\n",
    "    avg_score = 0\n",
    "\n",
    "    # do multiple runs for the same weights to get a good average score\n",
    "    for run in range(runs):\n",
    "\n",
    "        # reset environment\n",
    "        observation = env.reset()\n",
    "        score = 0\n",
    "\n",
    "        # do a single simulation run\n",
    "        action = env.action_space.sample()  # random first step\n",
    "        for i in range(simulation_len):\n",
    "\n",
    "            # do the action and get new observations\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "\n",
    "            # determine action\n",
    "            action = int(np.argmax((w * observation).mean(axis=1)))\n",
    "\n",
    "            # stop when done\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        avg_score += score / runs\n",
    "\n",
    "    # update best score and weights\n",
    "    if avg_score > best_score:\n",
    "        print(f'\\rfound new best score: {avg_score}')\n",
    "        best_score = avg_score\n",
    "        w_best = w\n",
    "\n",
    "# clean up\n",
    "env.close()\n",
    "\n",
    "# print results\n",
    "print('\\rbest score:', best_score)\n",
    "print(w_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w_best # or alternatively random: np.random.uniform(size=n)\n",
    "\n",
    "# setup environment\n",
    "env = gym.make(env_name)\n",
    "observation = env.reset()\n",
    "score = 0\n",
    "\n",
    "# do a simulation run with visualization for the best weights\n",
    "action = env.action_space.sample()\n",
    "for i in range(simulation_len):\n",
    "    \n",
    "    # do the action and get new observations\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "\n",
    "    # determine action\n",
    "    action = int(np.argmax((w * observation).mean(axis=1)))\n",
    "\n",
    "    # update plot and log\n",
    "    env.render(mode = \"human\")\n",
    "    print(f'\\rstep {i+1}, reward: {reward}, done: {done}, action: {action}', end=' '*20)\n",
    "    # print(f'\\rstep {i+1}, observation {observation}, reward: {reward}, done: {done}, action: {action}', end=' '*20)\n",
    "    time.sleep(0.02)\n",
    "\n",
    "    # stop when done\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# log result score\n",
    "print(f'\\ntotal reward: {score}')\n",
    "\n",
    "# clean up\n",
    "time.sleep(1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "It seems like 4 neurons with 8 inputs each, without bias and `argmax` for action determination are enough to solve this.\n",
    "\n",
    "However, it takes quite some time to find good parameters with just random tries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('gym-test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8e12c08af21acbec4bca7a654be22fcd0578cd9fee84ad2485d232941c1a4c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
